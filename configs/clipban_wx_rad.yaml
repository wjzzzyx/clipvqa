NAME: 'WXCLIPfix_seq.WXCLIPfix_seq.prompt10.BAN.QAS'
OUTPUT_DIR: './output/clipban/VQARAD'
# CPU_MODE: False
SEED: 88

MODEL: "clipban"

DATASET:
  DATASET: "RAD"
  DATA_DIR: "./data/data_rad"
#   DATA_TYPE: 'jpg'

LOSS:
  LOSS_TYPE: 'BCELogits'

TRAIN:
  BATCH_SIZE: 16
  N_EPOCH: 1000
#   NUM_WORKERS: 2 
#   RESUME: False 
#   INPUT_SNAPSHOT: "" 
  OPTIMIZER:
    TYPE: 'ADAMX'
    BASE_LR: 1e-3
    MOMENTUM_CNN: 0.05
    EPS_CNN: 1e-5
#   ACTIVATION: 'relu'
#   DROPOUT: 0.5
  ATTENTION:
#     MODE: 'BAN'
    GLIMPSE: 10
#     USE_COUNTER: False
#     NUM_STACKS: 2    ## For SAN
  QUESTION:
#     RNN: 'GRU'
    LENGTH: 24
#     TFIDF: True
#     CAT: True
    HID_DIM: 768   ## Dim of joint semantic features 1024
#     CLS_HID_DIM: 512
    PREFIX_LEN: 10
    COND_PROMPT: False
    EMBED_TYPE: "seq"
  VISION:
    V_DIM: 2048
    AUTOENCODER: False 
#     AE_PATH: "pretrained_ae.pth"
#     AE_ALPHA: 0.001
    MAML: False
    CLIP: True
#     CLIP_PATH: "/mnt/data2/yixiao/PubMedCLIP/pretrained/PubMedCLIP_RN50.pth"
    CLIP_VISION_ENCODER: "RN50"
    OTHER_MODEL: False
    PREFIX_LEN: 10
    COND_PROMPT: False
    HID_DIM: 768
    POOL_DIM: 1024
    EMBED_TYPE: "pool"
  ANSWER:
    PREFIX_LEN: 10
    HID_DIM: 768

  CLIP_TYPE: "weixiong"    # origin, weixiong
#   CLIP_PATH: "openai/clip-vit-base-patch32"
  IMAGE_ENCODER_PATH: "/mnt/sde/yixiao/vlp/Encoder/ImageEncoder/ImageEncoder.bin"
  TEXT_ENCODER_PATH: "/mnt/sde/yixiao/vlp/Encoder/TextEncoder/TextEncoder.bin"
#   # TEXT_ENCODER_PATH: "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
  QAS: 'dot'    # [dot, scaled dot, cosine, scaled cosine]

# TRANSFORMS:
#   TRAIN_TRANSFORMS: ("random_resized_crop", "random_horizontal_flip")
#   TEST_TRANSFORMS: ("shorter_resize_for_crop", "center_crop")
#   
TEST:
  BATCH_SIZE: 8 
#   NUM_WORKERS: 4
